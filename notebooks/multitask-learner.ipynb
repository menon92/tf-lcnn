{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('..')\n",
    "from lcnn.config import M\n",
    "\n",
    "print(tf.__version__)\n",
    "print(torch.__version__)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.1\n",
      "1.9.0+cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class MultitaskHead(nn.Module):\n",
    "    def __init__(self, input_channels, num_class):\n",
    "        super(MultitaskHead, self).__init__()\n",
    "\n",
    "        m = int(input_channels / 4)\n",
    "        heads = []\n",
    "        for output_channels in sum(M.head_size, []):\n",
    "            heads.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(input_channels, m, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(m, output_channels, kernel_size=1),\n",
    "                )\n",
    "            )\n",
    "        self.heads = nn.ModuleList(heads)\n",
    "        assert num_class == sum(sum(M.head_size, []))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def cross_entropy_loss(logits, positive):\n",
    "    nlogp = -F.log_softmax(logits, dim=0)\n",
    "    return (positive * nlogp[1] + (1 - positive) * nlogp[0]).mean(2).mean(1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def sigmoid_l1_loss(logits, target, offset=0.0, mask=None):\n",
    "    logp = torch.sigmoid(logits) + offset\n",
    "    loss = torch.abs(logp - target)\n",
    "    if mask is not None:\n",
    "        w = mask.mean(2, True).mean(1, True)\n",
    "        w[w == 0] = 1\n",
    "        loss = loss * (mask / w)\n",
    "\n",
    "    loss = loss.mean(2).mean(1)\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultitaskLearner(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(MultitaskLearner, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        head_size = M.head_size\n",
    "        self.num_class = sum(sum(head_size, []))\n",
    "        self.head_off = np.cumsum([sum(h) for h in head_size])\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        image = input_dict[\"image\"]\n",
    "        outputs, feature = self.backbone(image)\n",
    "        result = {\"feature\": feature}\n",
    "        batch, channel, row, col = outputs[0].shape\n",
    "\n",
    "        T = input_dict[\"target\"].copy()\n",
    "\n",
    "        n_jtyp = T[\"jmap\"].shape[1]\n",
    "\n",
    "        # switch to CNHW\n",
    "        for task in [\"jmap\"]:\n",
    "            T[task] = T[task].permute(1, 0, 2, 3)\n",
    "        for task in [\"joff\"]:\n",
    "            T[task] = T[task].permute(1, 2, 0, 3, 4)\n",
    "            \n",
    "\n",
    "        offset = self.head_off # [2 3 5]\n",
    "        loss_weight = M.loss_weight\n",
    "        losses = []\n",
    "        for stack, output in enumerate(outputs):\n",
    "            # 5 x N x H X W\n",
    "            output = output.transpose(0, 1).reshape([-1, batch, row, col]).contiguous()\n",
    "            jmap = output[0 : offset[0]].reshape(n_jtyp, 2, batch, row, col)\n",
    "            lmap = output[offset[0] : offset[1]].squeeze(0)\n",
    "            joff = output[offset[1] : offset[2]].reshape(n_jtyp, 2, batch, row, col)\n",
    "            \n",
    "            if stack == 0:\n",
    "                result[\"preds\"] = {\n",
    "                    \"jmap\": jmap.permute(2, 0, 1, 3, 4).softmax(2)[:, :, 1],\n",
    "                    \"lmap\": lmap.sigmoid(),\n",
    "                    \"joff\": joff.permute(2, 0, 1, 3, 4).sigmoid() - 0.5,\n",
    "                }\n",
    "                if input_dict[\"mode\"] == \"testing\":\n",
    "                    return result\n",
    "\n",
    "            L = OrderedDict()\n",
    "            L[\"jmap\"] = sum(\n",
    "                cross_entropy_loss(jmap[i], T[\"jmap\"][i]) for i in range(n_jtyp) # n_jtype {R, G, B} or gray\n",
    "            )\n",
    "            L[\"lmap\"] = (\n",
    "                F.binary_cross_entropy_with_logits(lmap, T[\"lmap\"], reduction=\"none\")\n",
    "                .mean(2)\n",
    "                .mean(1)\n",
    "            )\n",
    "            L[\"joff\"] = sum(\n",
    "                sigmoid_l1_loss(joff[i, j], T[\"joff\"][i, j], -0.5, T[\"jmap\"][i])\n",
    "                for i in range(n_jtyp)\n",
    "                for j in range(2)\n",
    "            )\n",
    "            for loss_name in L:\n",
    "                L[loss_name].mul_(loss_weight[loss_name])\n",
    "            losses.append(L)\n",
    "        result[\"losses\"] = losses\n",
    "        return result\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3a4705feea52fcbee62e5c67e8ab7219e40d1a70e6db814be6b8c49c5caaa02a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}